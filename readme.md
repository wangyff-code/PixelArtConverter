
# ğŸ¨ AI Pixel Art Converter (Pixelformer)

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange?style=flat-square&logo=pytorch)
![ONNX Runtime](https://img.shields.io/badge/ONNX-Runtime-blueviolet?style=flat-square&logo=onnx)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

**A local desktop app transforming images into high-quality Pixel Art using Deep Learning.**  
**ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æœ¬åœ°æ¡Œé¢åº”ç”¨ï¼Œå°†å›¾ç‰‡è½¬æ¢ä¸ºé«˜è´¨é‡çš„åƒç´ ç”»ã€‚**

[Features (ç‰¹æ€§)](#-features-åŠŸèƒ½ç‰¹æ€§) â€¢ [Installation (å®‰è£…)](#-installation--usage-å®‰è£…ä¸ä½¿ç”¨) â€¢ [Tech Stack (æŠ€æœ¯æ ˆ)](#-technical-architecture-æŠ€æœ¯æ¶æ„) â€¢ [Gallery (ç”»å»Š)](#-gallery-æ•ˆæœå±•ç¤º)

</div>

---

## ğŸ“– Introduction (ç®€ä»‹)

**English**  
This is not a simple downsampling filter. **Pixelformer** is an AI-powered tool that understands image structure and lighting. It uses a custom **Transformer + CNN** architecture trained with GANs to generate crisp, aesthetically pleasing pixel art with coherent color palettes. The application runs locally using **ONNX Runtime** and **PyWebView**, requiring no expensive GPU for inference.

**ä¸­æ–‡**  
è¿™ä¸æ˜¯ä¸€ä¸ªç®€å•çš„é©¬èµ›å…‹æ»¤é•œã€‚**Pixelformer** æ˜¯ä¸€ä¸ªç”± AI é©±åŠ¨çš„å·¥å…·ï¼Œå®ƒèƒ½ç†è§£å›¾åƒçš„ç»“æ„å’Œå…‰å½±ã€‚é¡¹ç›®ä½¿ç”¨äº†è‡ªå®šä¹‰çš„ **Transformer + CNN** æ··åˆæ¶æ„ï¼Œå¹¶é€šè¿‡ GANï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ã€å…·æœ‰ç¾æ„Ÿçš„åƒç´ ç”»ä»¥åŠè¿è´¯çš„è‰²æ¿ã€‚åº”ç”¨ç¨‹åºåŸºäº **ONNX Runtime** å’Œ **PyWebView** æ„å»ºï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€æ˜‚è´µçš„æ˜¾å¡å³å¯å¿«é€Ÿæ¨ç†ã€‚

---

## âœ¨ Features (åŠŸèƒ½ç‰¹æ€§)

| Feature | Description (English) | è¯´æ˜ (ä¸­æ–‡) |
| :--- | :--- | :--- |
| **ğŸ§  AI Core** | Powered by **Paletteformer**, trained with Perceptual & GAN losses. | å†…ç½® **Paletteformer** æ¨¡å‹ï¼ŒåŸºäºæ„ŸçŸ¥æŸå¤±å’Œ GAN è®­ç»ƒã€‚ |
| **âš¡ Local Inference** | Runs on CPU via ONNX. No internet needed. | åŸºäº ONNX çš„ CPU æ¨ç†ï¼Œæ— éœ€è”ç½‘ï¼Œé€Ÿåº¦å¿«ã€‚ |
| **ğŸ¨ Smart Palette** | Extracts a coherent 32-color palette automatically. | è‡ªåŠ¨æå–å¹¶ç”Ÿæˆåè°ƒçš„ 32 è‰²è‰²å¡ã€‚ |
| **ğŸ›ï¸ Creative Control** | Adjust **Temperature** (sharpness) and **Noise** (texture). | å¯è°ƒèŠ‚**æ¸©åº¦**ï¼ˆæ§åˆ¶é”åº¦ï¼‰å’Œ**å™ªå£°**ï¼ˆå¢åŠ çº¹ç†ç»†èŠ‚ï¼‰ã€‚ |
| **ğŸ–¥ï¸ Desktop UI** | Lightweight GUI with Drag & Drop, Zoom, and Pan support. | è½»é‡çº§ GUIï¼Œæ”¯æŒæ‹–æ‹½ä¸Šä¼ ã€æ»šè½®ç¼©æ”¾å’Œå¹³ç§»é¢„è§ˆã€‚ |
| **ğŸ“ Multi-Res** | Supports 128x128 and 256x256 output resolutions. | æ”¯æŒ 128x128 å’Œ 256x256 ä¸¤ç§è¾“å‡ºåˆ†è¾¨ç‡ã€‚ |

---

## ğŸ–¼ï¸ Gallery (æ•ˆæœå±•ç¤º)

<div align="center">
  <img src="example/face.png"width="800">
</div>

| Input (åŸå›¾) | Pixel Art (åƒç´ ç”» 128 ) | Pixel Art (åƒç´ ç”» 256 )  |
| :---: | :---: | :---: |
| <img src="example/o1.png" width="200"> | <img src="example/d1_128.png" width="200"> |<img src="example/d1_256.png" width="200"> |
| <img src="example/o2.png" width="200"> | <img src="example/d2_128.png" width="200"> |<img src="example/d2_256.png" width="200"> |
---

## ğŸ›  Technical Architecture (æŠ€æœ¯æ¶æ„)

### Model: Paletteformer
The core model combines the best of CNNs and Transformers:
1.  **Content Encoder**: A ResNet-style encoder extracts multi-scale features.
2.  **Contextual Palette Extractor**: Uses **Self-Attention** to generate adaptive color queries based on input noise and image features.
3.  **Pixel Decoder**: Reconstructs the image pixel-by-pixel using the learned palette.

### Training Strategy (è®­ç»ƒç­–ç•¥)
We employ a 3-stage training pipeline to ensure quality:
*   **Stage 1 (Structure)**: L1 Loss for basic shape reconstruction.
*   **Stage 2 (Color)**: **Chamfer Distance Loss** in CIELAB space to enforce accurate color perception.
*   **Stage 3 (Refinement)**: **GAN (Hinge Loss)** + Hard Gumbel-Softmax to sharpen pixels and remove blur.

### æ ¸å¿ƒæ¨¡å‹ï¼šPaletteformer
æ ¸å¿ƒæ¨¡å‹ç»“åˆäº† CNN å’Œ Transformer çš„ä¼˜åŠ¿ï¼š
1.  **å†…å®¹ç¼–ç å™¨**ï¼šä½¿ç”¨ç±» ResNet ç»“æ„æå–å¤šå°ºåº¦å›¾åƒç‰¹å¾ã€‚
2.  **ä¸Šä¸‹æ–‡è‰²æ¿æå–å™¨**ï¼šåˆ©ç”¨**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼Œæ ¹æ®è¾“å…¥å™ªå£°å’Œå›¾åƒç‰¹å¾ç”Ÿæˆè‡ªé€‚åº”çš„é¢œè‰²æŸ¥è¯¢å‘é‡ã€‚
3.  **åƒç´ è§£ç å™¨**ï¼šåˆ©ç”¨å­¦ä¹ åˆ°çš„è‰²æ¿é€åƒç´ é‡æ„å›¾åƒã€‚

### è®­ç»ƒç­–ç•¥
æˆ‘ä»¬é‡‡ç”¨äº†ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ä»¥ä¿è¯è´¨é‡ï¼š
*   **é˜¶æ®µ 1 (ç»“æ„)**ï¼šä½¿ç”¨ L1 æŸå¤±å‡½æ•°å­¦ä¹ åŸºæœ¬å½¢çŠ¶é‡æ„ã€‚
*   **é˜¶æ®µ 2 (è‰²å½©)**ï¼šåœ¨ CIELAB ç©ºé—´ä½¿ç”¨**å€’è§’è·ç¦»æŸå¤± (Chamfer Distance Loss)**ï¼Œç¡®ä¿è‰²å½©æ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚
*   **é˜¶æ®µ 3 (ç²¾ç»†åŒ–)**ï¼šå¼•å…¥ **GAN (Hinge Loss)** å’Œ Hard Gumbel-Softmaxï¼Œæ¶ˆé™¤æ¨¡ç³Šï¼Œä½¿åƒç´ ç‚¹æ›´åŠ é”åˆ©æ¸…æ™°ã€‚

---

## ğŸ“¦ Installation & Usage (å®‰è£…ä¸ä½¿ç”¨)

### Prerequisites (å‰ç½®è¦æ±‚)
*   Python 3.8+
*   pip

### 1. Clone the repository (å…‹éš†ä»“åº“)
```bash
git clone https://github.com/yourusername/pixel-art-ai.git
cd pixel-art-ai
```

### 2. Install Dependencies (å®‰è£…ä¾èµ–)
```bash
pip install -r requirements.txt
```
*(Dependencies include: `pywebview`, `onnxruntime`, `numpy`, `Pillow`)*

### 3. Setup Models (å‡†å¤‡æ¨¡å‹)
Place your trained ONNX models in the `onnx_models` folder.
è¯·å°†è®­ç»ƒå¯¼å‡ºå¥½çš„ ONNX æ¨¡å‹æ”¾å…¥ `onnx_models` æ–‡ä»¶å¤¹ã€‚
```text
project_root/
â”œâ”€â”€ onnx_models/
â”‚   â”œâ”€â”€ pixelart_128.onnx
â”‚   â””â”€â”€ pixelart_256.onnx
```

### 4. Run the App (è¿è¡Œåº”ç”¨)
```bash
python app.py
```

---

## ğŸ“‚ Project Structure (é¡¹ç›®ç»“æ„)

```text
.
â”œâ”€â”€ app.py                # Main Application Entry (GUI Logic) / ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ index.html            # Frontend UI (HTML/CSS/JS) / å‰ç«¯ç•Œé¢
â”œâ”€â”€ onnx_models/          # Model Storage / æ¨¡å‹å­˜æ”¾ç›®å½•
â””â”€â”€ requirements.txt      # Dependencies / ä¾èµ–åˆ—è¡¨
```

---

## ğŸ¤ Contributing (è´¡çŒ®)

Contributions are welcome! If you have ideas for better loss functions or UI improvements, feel free to open an issue or PR.
æ¬¢è¿è´¡çŒ®ä»£ç ï¼å¦‚æœä½ å¯¹æŸå¤±å‡½æ•°æ”¹è¿›æˆ– UI ä¼˜åŒ–æœ‰ä»»ä½•æƒ³æ³•ï¼Œæ¬¢è¿æäº¤ Issue æˆ– PRã€‚

1.  Fork the Project
2.  Create your Feature Branch (`git checkout -b feature/NewFeature`)
3.  Commit your Changes (`git commit -m 'Add some NewFeature'`)
4.  Push to the Branch (`git push origin feature/NewFeature`)
5.  Open a Pull Request

---

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

---

<div align="center">
  <sub>Built with â¤ï¸ by [Your Name]</sub>
</div>
